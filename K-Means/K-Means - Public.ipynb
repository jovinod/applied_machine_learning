{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# {About}\n",
    "\n",
    "* The KMeans algorithm clusters data by trying to separate samples in n groups of *equal variance*, minimizing a criterion known as the inertia or *within-cluster sum-of-squares*. This algorithm requires the number of clusters to be specified. It scales well to large number of samples and has been used across a large range of application areas in many different fields.\n",
    "    * The *equal variance* here is an assumption. K-Means inherently assumes that the variance across each group is constant. In fact the variation across each dimension is also constant. This is also the reason it results in Spherical, Convex or Isotrpoic clusters \n",
    "    * The *within-cluster sum-of-squares* is the overall summation of distance of each point to its centroid\n",
    "    * K-means is often referred to as **Lloyd’s algorithm**\n",
    "    * K-means is equivalent to the expectation-maximization algorithm with a small, all-equal, diagonal covariance matrix.\n",
    "\n",
    "*An arial view of cluster of localities in Dehradun*\n",
    "![Arial](Cluster.jpg \"Arial View of cluster of localities in Dehradun\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# {Intuition}\n",
    "* The k-means algorithm divides a set of **N** samples **X** into **K** disjoint clusters **C**, each described by the mean <math><msub><mi>&#x3BC;</mi><mi>(j)</mi></msub></math>\n",
    " of the samples in the cluster. The means are commonly called the cluster “centroids”; \n",
    "    * Note that they are not, in general, points from **X**, although they live in the same space. The reason behind this is that after the first iteration of centroid computation, the resulting centroid is a new data point which may or may not match any existing data points in the sample.\n",
    "* In basic terms, the algorithm has three steps. The first step chooses the initial centroids, with the most basic method being to choose  samples from the dataset . After initialization, K-means consists of looping between the two other steps. The first step assigns each sample to its nearest centroid. The second step creates new centroids by taking the mean value of all of the samples assigned to each previous centroid. The difference between the old and the new centroids are computed and the algorithm repeats these last two steps until this value is less than a threshold. In other words, it repeats until the centroids do not move significantly.\n",
    "    ![KMeans](kmeans.gif \"k-means\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# {Unsupervised}\n",
    "* K_Means is an unsupervised learning which helps find previously unknown patterns in data set without pre-existing labels. It is the latter of one of the two of the main methods used in unsupervised learning principal component and cluster analysis. Simply put, you may have at some point used mean as a way to find aggregate representation of the data, K-Means extends that by increasing the number number of centroids to K. Except for the hyper-parameter no prior information is required to explore the unknown patterns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# {Descriptive}\n",
    "* Machine learning algorithms can be clssified into 2 broad categories\n",
    "    * Descriptive: centered around the process that generated the data, it highlights the stucture and pattern in the data. Thes models are helpful when no prior information about the data is available and the nature of research is exploratory\n",
    "    * Discriminative: as the name suggests is centered around finding the difference between classes or categories. These models focus on differentiating the data and are also mostly the supervised class of algorithms\n",
    "\n",
    "K-Means models the distribution of the data and hence falls in the Descriptive or Generative models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# {Clustering}\n",
    "* K-Means falls in the category of cluster analysis or clustering. \n",
    "* Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). It is a main task of exploratory data mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# {Parametric}\n",
    "* K-Means is a parameteric method, where the objective is to optimize the model for finding the best parameters given the data\n",
    "* With a generic formulation of K-Means, we can infer which cluster each data point belongs. This can be stated as given the $x$ data coming from a distribution $D$, find the maximum probability that it belongs to cluster $k$.\n",
    "\\begin{equation*}\n",
    "z_i = argmax_k P(z_i = k | x_i, D)\n",
    "\\end{equation*}\n",
    "* In order to understand the parameters for K-Means we need to expand this formulation to write the specific objective funtion. In this objective function, the machine is trying to learn the parameter $\\mu$ (centroids) for which the sum of the intra cluster distance between each point and its centroid is the least\n",
    "\\begin{equation*}\n",
    "Objective Function = \\sum_{k=1}^K \\sum_{C(i)=k}^n N_{ik} (x_i - \\mu_{k})^2\n",
    "\\end{equation*}\n",
    "* There is also a latent parameter $N_{ik}$ which takes a binary value 0 or 1 based on whether the data point belongs to that centroid or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# {Data, Parameter & Hyper-Parameter}\n",
    "* We will build further from our previous formulation to identify what are the data elements, parameters and hyper-parameter(s) for K-Means\n",
    "\\begin{equation*}\n",
    "Objective Function = \\sum_{k=1}^K \\sum_{C(i)=k}^n N_{ik} (x_i - \\mu_{k})^2\n",
    "\\end{equation*}\n",
    "* From the formulation above we can identify the following\n",
    "    * $x_{i}$ is the input data which can be one or multi-dimensional. For text and image processing that data can be very high dimensional and hence may need techniques like PCA to avoid the curse of dimensionality\n",
    "    * $\\mu_{k}$ is the parameter that the machine is trying to optimize for based on the input data $x_{i}$\n",
    "    * Since K-Means is a hard clsutering, a latent parameter $N_{ik}$ helps assigning a value of 1 for centroid k to data point $x_i$ and 0 to all other centroid $K <> k$\n",
    "    * Finally K-Means also has a hyper-parameter which is specified in the form of K. K represents the number of centroids that exist in the input data. This parameter needs to be provided as an input to the formulation. In order to identify the optimum value of K, we can further fomulate an objective function in the following form. This can be stated as, given the data $x$ coming from distribution $D$, find the highest probability of $K$ being $k$\n",
    "    \\begin{equation*}\n",
    "    K = argmax_k P(K=k | x_i, D)\n",
    "    \\end{equation*}\n",
    "    * Hyper-paramete tuning as stated above is an objective function in itself that we will cover in the next section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# {Hyper-parameter Tuning}\n",
    "* There are 3 prominent ways to identify the best hyper-parameter for the input data\n",
    "    * **Elbow Method**: The idea behind elbow method is to run k-means clustering on a given dataset for a range of values of k (num_clusters, e.g k=1 to 10), and for each value of k, calculate sum of squared errors (SSE).\n",
    "\n",
    "    After that, plot a line graph of the SSE for each value of k. If the line graph looks like an arm, the \"elbow\" on the arm is the value of optimal k (number of cluster). Here, we want to minimize SSE. SSE tends to decrease toward 0 as we increase k (and SSE is 0 when k is equal to the number of data points in the dataset, because then each data point is its own cluster, and there is no error between it and the center of its cluster). \n",
    "    \n",
    "    SSE is the sum squared differences between each observation and it's group mean. It can be used as a measure of variation within the cluster. If all data points within a cluster are identical the SSE would be equal to 0\n",
    "\\begin{equation*}\n",
    "SSE = \\sum_{k=1}^n (x_i - \\mu)^2\n",
    "\\end{equation*}\n",
    "\n",
    "    At each stage of the cluster analysis the total SSE is minimized with $SSE{_t} = SSE{_1} + SSE{_2} + SSE{_3} + ... + SSE{_n}$. At initial stage when each data point is in it's own cluster the SSE is ofcourse 0. So the goal is to choose a small value of k that still has a low SSE, and the elbow usually represents where we start to have diminishing returns by increasing k. Considered to be an elementary method that may not provide the required accuracy.\n",
    "    \n",
    "    * **Silhouette Score**: The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from −1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters. For data point $i$ in cluster $C_i$\n",
    "\\begin{equation*}\n",
    "{\\displaystyle a(i)={\\frac {1}{|C_{i}|-1}}\\sum _{j\\in C_{i},i\\neq j}d(i,j)}\n",
    "\\end{equation*}\n",
    "be the mean distance between $i$ and all other data points in the same cluster, where $d(i,j)$ is the distance between data points $i$ and $j$ in the cluster $C_i$ (we divide by $|C_{i}|-1$ because we do not include the distance $d(i,i)$ in the sum). We can interpret $a(i)$ as a measure of how well $i$ is assigned to its cluster (the smaller the value, the better the assignment).\n",
    "\n",
    "    We then define the mean dissimilarity of point $i$ to some cluster $C$ as the mean of the distance from $i$ to all points in $C$ (where $C\\neq C_{i}$). For each data point $i$ in $C_{i}$, we now define\n",
    "    \\begin{equation*}\n",
    "    {\\displaystyle b(i)=\\min _{k\\neq i}{\\frac {1}{|C_{k}|}}\\sum _{j\\in C_{k}}d(i,j)}\n",
    "    \\end{equation*}\n",
    "\n",
    "    to be the smallest (hence the $min$  operator in the formula) mean distance of $i$ to all points in any other cluster, of which $i$ is not a member. The cluster with this smallest mean dissimilarity is said to be the \"neighboring cluster\" of $i$ because it is the next best fit cluster for point $i$.\n",
    "\n",
    "    We now define a silhouette (value) of one data point $i$\n",
    "    \\begin{equation*}\n",
    "    {\\displaystyle s(i)={\\frac {b(i)-a(i)}{\\max\\{a(i),b(i)\\}}}}, if {\\displaystyle |C_{i}|>1}{\\displaystyle |C_{i}|>1}\n",
    "    \\end{equation*}\n",
    "    \n",
    "    and\n",
    "    \\begin{equation*}\n",
    "    {\\displaystyle s(i)=0}, if {\\displaystyle |C_{i}|=1}{\\displaystyle |C_{i}|=1}\n",
    "    \\end{equation*}\n",
    "    \n",
    "    Which can be also written as:\n",
    "\n",
    "    \\begin{equation*}\n",
    "    {\\displaystyle s(i)={\\begin{cases}1-a(i)/b(i),&{\\mbox{if }}a(i)<b(i)\\\\0,&{\\mbox{if }}a(i)=b(i)\\\\b(i)/a(i)-1,&{\\mbox{if }}a(i)>b(i)\\\\\\end{cases}}}\n",
    "    \\end{equation*}\n",
    "\n",
    "    From the above definition it is clear that\n",
    "    \\begin{equation*}\n",
    "     -1 \\le s(i) \\le 1\n",
    "    \\end{equation*}\n",
    "\n",
    "     Also, note that score is 0 for clusters with size = 1. This constraint is added to prevent the number of clusters from increasing significantly.\n",
    "\n",
    "    For $s(i)$ to be close to 1 we require $a(i) << b(i)$. As $a(i)$ is a measure of how dissimilar $i$ is to its own cluster, a small value means it is well matched. Furthermore, a large {\\displaystyle $b(i)$ implies that $i$ is badly matched to its neighbouring cluster. Thus an {\\displaystyle $s(i)$ close to one means that the data is appropriately clustered. If $s(i)$ is close to negative one, then by the same logic we see that $i$ would be more appropriate if it was clustered in its neighbouring cluster. An $s(i)$ near zero means that the datum is on the border of two natural clusters.\n",
    "\n",
    "    The mean $s(i)$ over all points of a cluster is a measure of how tightly grouped all the points in the cluster are. Thus the mean $s(i)$ over all data of the entire dataset is a measure of how appropriately the data have been clustered. If there are too many or too few clusters, as may occur when a poor choice of $k$ is used in the clustering algorithm (e.g.: k-means), some of the clusters will typically display much narrower silhouettes than the rest. Thus silhouette plots and means may be used to determine the natural number of clusters within a dataset. One can also increase the likelihood of the silhouette being maximized at the correct number of clusters by re-scaling the data using feature weights that are cluster specific.\n",
    "\n",
    "    Kaufman et al. introduced the term silhouette coefficient for the maximum value of the mean $s(i)$ over all data of the entire dataset.\n",
    "    \\begin{equation*}\n",
    "    {\\displaystyle SC=\\max _{k}{\\tilde {s}}\\left(k\\right)}\n",
    "    \\end{equation*}\n",
    "    \n",
    "    Where $\\tilde {s}(k)$ represents the mean $s(i)$ over all data of the entire dataset for a specific number of clusters $k$.\n",
    "    * **Gap Statistics**: The approach can be applied to any clustering method (i.e. K-means clustering, hierarchical clustering). The gap statistic compares the total intracluster variation for different values of k with their expected values under null reference distribution of the data (i.e. a distribution with no obvious clustering). The reference dataset is generated using Monte Carlo simulations of the sampling process. That is, for each variable $x_i$ in the data set we compute its range $[min(x_i),max(x_j)]$ and generate values for the $n$ points uniformly from the interval min to max.\n",
    "    \n",
    "    For the observed data and the the reference data, the total intracluster variation is computed using different values of $k$. The gap statistic for a given $k$ is defined as follow:\n",
    "    \\begin{equation*}\n",
    "    Gap_n(k)= E_nlog(W_k) - log(W_k)\n",
    "    \\end{equation*}\n",
    "    \n",
    "    $W_k$ is defined as the intracluster variation expressed as \n",
    "    \\begin{equation*}\n",
    "    W_k= \\sum_{k=1}^n (x_i - \\mu)^2\n",
    "    \\end{equation*}    \n",
    "    \n",
    "    Where $E_n$ denotes the expectation under a sample size n from the reference distribution. E∗n is defined via bootstrapping B) by generating B copies of the reference datasets and, by computing the average $log(W_k)$. The gap statistic measures the eviation of the observed $W_k$ value from its expected value under the null hypothesis. The estimate of the optimal clusters $k$ ill be the value that maximizes $Gap_n(k)$. This means that the clustering structure is far away from the uniform distribution of points.\n",
    "    \n",
    "    In short, the algorithm involves the following steps:\n",
    "        * Cluster the observed data, varying the number of clusters from $k=1,…,kmax$, and compute the corresponding $W_k$.\n",
    "        * Generate B reference data sets and cluster each of them with varying number of clusters $k=1,…,kmax$. Compute the estimated gap statistics presented above\n",
    "            * Let $w= (1/B)\\sum_{k=1}^blog(W_kb)$, compute the standard deviation \n",
    "            \n",
    "                $sd_k=\\sqrt{(1/b)\\sum_{k=1}^b(log(W_k)− w)^2}$ and define $s_k=sd_k×\\sqrt{1+1/B}$.\n",
    "                \n",
    "        * Choose the number of clusters as the smallest $k$ such that\n",
    "            $Gap_{k} ≥ Gap_{k+1} − s_{k+1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# {Non-Detrministic}\n",
    "K-Means is an iterative process where at each step in the phase the centroids are determined and subsequently each data point is assigned to the centroid closest to it using a form of distance (eucledean). The cluster centroids can be assigned using various methods e.g. random assignment, farthest first, aggregated random assignment to avoid local minima etc. Due to the iterative nature of the algrithm and a randomness in the assignment of the centroids, k-means is non-detrminitsic in nature. It means that in two different runs of the algorithms the assigment of a data point may vary between 2 centroids. Below is the functional specification of K-Means from sklearn. As you can see parameters $init$ defines how the cluster centroids are chosen $n\\_init$ defines how many iterations of the algorithm would run for each instance and $random\\_state$ defining random number seed would be used to initilize the centroid.\n",
    "\n",
    "```python\n",
    "class sklearn.cluster.KMeans(n_clusters=8, init='k-means++', n_init=10, max_iter=300, tol=0.0001, precompute_distances='auto', verbose=0, random_state=None, copy_x=True, n_jobs=None, algorithm='auto')[source]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# {Hard Association}\n",
    "The association of data point to a centroid in K-Means is hard. It means that during the ierative assignment as well final association, one data point is associated to only one centroid. This is different something like GMM (Gaussian Mixture Model) where a probability of association is assigned to data point for each centroid (in this case Gaussian). To translate this to K-means, the probability of association of a data point to a centroid lies at the extreme $P(C_k) = 1$ or $P(C_k) = 0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# {Functional Form}\n",
    "**Classic K-mean**\n",
    "In classic k-means, we seek to minimize a Euclidean distance between the cluster center and the members of the cluster. The intuition behind this is that the radial distance from the cluster-center to the element location should \"have sameness\" or \"be similar\" for all elements of that cluster.\n",
    "\n",
    "**Spherical K-means**\n",
    "In spherical k-means, the idea is to set the center of each cluster such that it makes both uniform and minimal the angle between components. The intuition is like looking at stars - the points should have consistent spacing between each other. That spacing is simpler to quantify as \"cosine similarity\", but it means there are no \"milky-way\" galaxies forming large bright swathes across the sky of the data.\n",
    "\n",
    "Think about vectors, the things you graph as arrows with orientation, and fixed length. It can be translated anywhere and be the same vector. The orientation of the point in the space (its angle from a reference line) can be computed using linear algebra, particularly the dot product.\n",
    "\n",
    "![Classic KMeans](Classical.png \"k-means\")\n",
    "\n",
    "If we move all the data so that their tail is at the same point, we can compare \"vectors\" by their angle, and group similar ones into a single cluster.\n",
    "\n",
    "![Spherical KMeans](Spherical.png \"k-means\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# {Objective Function & Derivation}\n",
    "In the previous section we looked at the objective function mentioned belown the previous section. In this section we will look at the derivation of the solution to this objective function for the parameters $\\mu$. While $N_{ik}$, the latent parameter is also a parameter to be determined, due to the iterative nature of K-Means, in this derivation $N_{ik}$ is known for the specific iteration.\n",
    "\n",
    "\\begin{equation*}\n",
    "J(\\mu, N_{ik}) = \\sum_{k=1}^K \\sum_{C(i)=k}^n N_{ik} (x_i - \\mu)^2\n",
    "\\end{equation*}\n",
    "\n",
    "**Step 1**: In order to solve the objective function for parameter $\\mu_{k^{\\prime}}$,  (mean of $k^{\\prime}$ cluster), we will differentiate the objective function to find the minima. The minima is required in order to find the least intra-cluster distance\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial J}{\\partial \\mu_{k^{\\prime}}} = \\sum_{k=k^{\\prime}} \\sum_{C(i)=k^{\\prime}}^n N_{ik^{\\prime}} (x_i - \\mu_{k^{\\prime}})^2\n",
    "\\end{equation*}\n",
    "\n",
    "**Step 2**: Since we are solving the objective function for one centroid $\\mu_{k^{\\prime}}$, the first summation is redundant. Since we are trying to find the minima the differentiation can be equated to 0\n",
    "\n",
    "\\begin{equation*}\n",
    "2 * \\sum_{C(i)=k^{\\prime}}^n N_{ik^{\\prime}} (x_i - \\mu_{k^{\\prime}}) = 0\n",
    "\\end{equation*}\n",
    "\n",
    "**Step 3**: Since we are solving the objective function for one centroid $\\mu_{k^{\\prime}}$, the first summation is redundant. Since we are trying to find the minima the differentiation can be equated to 0\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mu_{k^{\\prime}} = \\frac {\\sum_{C(i)=k^{\\prime}}^n N_{ik^{\\prime}} x_i} {N_{ik^{\\prime}}} \n",
    "\\end{equation*}\n",
    "\n",
    "**Step 4**: The above equation is very similar to the basic equation of mean, where the numerator constitutes the sum of all data points that belong to cluster $k^{\\prime}$ divided by the total number of data points in cluster $k^{\\prime}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# {Complexity}\n",
    "The complexity of K-Means for running a fixed number of iterations $t$ of the algorithm takes can be expressed as $O(t*k*n*d)$, for $n$ ($d$-dimensional) points, with $k$ centroids (or clusters). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# {Purity}\n",
    "The purity of a cluster is the measure of correctness of the association of a data point to it's centroid or cluster. This can be done using various metrics, however each one of them needs a labelled data to determine the purity\n",
    "\n",
    "* Adjusted Rand Index: The Rand Index computes a similarity measure between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings.The raw RI score is then “adjusted for chance” into the ARI score using the following scheme\n",
    "```python\n",
    "ARI = (RI - Expected_RI) / (max(RI) - Expected_RI)\n",
    "```\n",
    "* Completeness: Completeness metric of a cluster labeling given a ground truth. A clustering result satisfies completeness if all the data points that are members of a given class are elements of the same cluster. This metric is independent of the absolute values of the labels: a permutation of the class or cluster label values won’t change the score value in any way.\n",
    "* Homogeneity: Homogeneity metric of a cluster labeling given a ground truth. A clustering result satisfies homogeneity if all of its clusters contain only data points which are members of a single class. This metric is independent of the absolute values of the labels: a permutation of the class or cluster label values won’t change the score value in any way.\n",
    "* V-measure: The V-measure is the harmonic mean between homogeneity and completeness. This metric is independent of the absolute values of the labels: a permutation of the class or cluster label values won’t change the score value in any way.\n",
    "```python\n",
    "v = (1 + beta) * homogeneity * completeness\n",
    "     / (beta * homogeneity + completeness)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# {Visualization}\n",
    "There are multiple techniques that can be used to visualize the clusters especially in a multi-dimensional data set. Some of the methods are mentioned below. Each one needs a topic of it's own and will be explained seperately\n",
    "* PCA (Principal Component Analysis)\n",
    "* t-NSE (t-Distributed Stochastic Neighbor Embedding)\n",
    "\n",
    "Both are dimensionality reduction technique that helps in visualizing high dimensional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# {Application}\n",
    "\n",
    "* K-Means is helpful when you are trying to understand the underlying structure of the data. One of the classic examples that you would often find in marketing literature is Segmentation and Targeting. Underlying customer segment is identified using the K-Means and then based on the profiles adevrtisements can be created to target that segment. \n",
    "\n",
    "* One of the applications where I have used K-Means is clustering of failures in a DevOps Pipeline. Let's understand this scenario and how K-Means was applied to solve the problem\n",
    "    * **Scenario:** *In application development lifecycle, failures are inevitable.Developers typically spend a lot of time debugging, troubleshooting and looking for historical instances where similar issues were encountered and resolved. Additionally, patterns of failures are not evident when looking at them individually, but may highlight systemic issues when observed in bigger span of reference! As such it is important to understand the groups of failures that are impacting the pipeline*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# {Challenges}\n",
    "* Inertia, or within-cluster sum-of-squares criterion can be recognized as a measure of how internally coherent clusters are. It suffers from various drawbacks:\n",
    "    * Inertia makes the assumption that clusters are convex and isotropic, which is not always the case. It responds poorly to elongated clusters, or manifolds with irregular shapes.\n",
    "    * Given enough time, K-means will always converge, however this may be to a local minimum. This is highly dependent on the initialization of the centroids. As a result, the computation is often done several times, with different initializations of the centroids.\n",
    "    * Inertia is not a normalized metric: we just know that lower values are better and zero is optimal. But in very high-dimensional spaces, Euclidean distances tend to become inflated (this is an instance of the so-called “curse of dimensionality”). Running a dimensionality reduction algorithm such as Principal component analysis (PCA) prior to k-means clustering can alleviate this problem and speed up the computations.\n",
    "    ![Inertia](challenges.png \"inertia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# {Options}\n",
    "| Method name | Parameters | Scalability | Usecase | Geometry (metric used) |\n",
    "|-------------|------------|-------------|---------|------------------------|\n",
    "| K-Means     | number of clusters | Very large n_samples, medium n_clusters with | General-purpose, even cluster size, flat geometry, not too many clusters | Distances between points |\n",
    "| Affinity propagation | damping, sample preference | Not scalable with n_samples | Many clusters, uneven cluster size, non-flat geometry | Graph distance (e.g. nearest-neighbor graph) |\n",
    "| Mean-shift | bandwidth | Not scalable with n_samples | Many clusters, uneven cluster size, non-flat geometry | Distances between points |\n",
    "| Spectral clustering | number of clusters | Medium n_samples, small n_clusters | Few clusters, even cluster size, non-flat geometry | Graph distance (e.g. nearest-neighbor graph) |\n",
    "| Ward hierarchical clustering | number of clusters or distance threshold | Large n_samples and n_clusters | Many clusters, possibly connectivity constraints | Distances between points |\n",
    "| Agglomerative clustering | number of clusters or distance threshold, linkage type, distance | Large n_samples and n_clusters | Many clusters, possibly connectivity constraints, non Euclidean distances | Any pairwise distance |\n",
    "| DBSCAN | neighborhood size | Very large n_samples, medium n_clusters | Non-flat geometry, uneven cluster sizes | Distances between nearest points |\n",
    "| OPTICS | minimum cluster membership | Very large n_samples, large n_clusters | Non-flat geometry, uneven cluster sizes, variable cluster density | Distances between points |\n",
    "| Gaussian mixtures | many | Not scalable | Flat geometry, good for density estimation | Mahalanobis distances to  centers |\n",
    "| Birch | branching factor, threshold, optional global clusterer. | Large n_clusters and n_samples | Large dataset, outlier removal, data reduction. | Euclidean distance between points |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
